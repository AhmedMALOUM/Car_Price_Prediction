# Importation des librairies
import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.preprocessing import OneHotEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib
from pymongo import MongoClient

# ðŸ“Œ Connexion Ã  MongoDB et rÃ©cupÃ©ration des donnÃ©es
client = MongoClient("mongodb://localhost:27017/")
db = client["car_db"]
collection = db["cars"]
data = list(collection.find({}, {"_id": 0}))  # Exclure l'ID MongoDB
df = pd.DataFrame(data)

# ðŸ“Œ Nettoyage des donnÃ©es
# Suppression des valeurs aberrantes
df = df[(df["price"] > 500) & (df["price"] < 200000)]

# CrÃ©ation d'une nouvelle colonne "Ã¢ge du vÃ©hicule"
df["car_age"] = 2025 - df["model_year"]
df = df.drop(columns=["model_year"], errors="ignore")  # Supprime "year" aprÃ¨s crÃ©ation de car_age

# ðŸ“Œ SÃ©parer les colonnes numÃ©riques et catÃ©gorielles
cat_cols = df.select_dtypes(include=["object"]).columns
num_cols = df.select_dtypes(include=["int64", "float64"]).columns

# ðŸ“Œ One-Hot Encoding des variables catÃ©gorielles
df = pd.get_dummies(df, columns=cat_cols, drop_first=True)

# ðŸ“Œ DÃ©finition des features et de la target
X = df.drop(columns=["price"])  # On prÃ©dit le prix
y = df["price"]

# ðŸ“Œ SÃ©parer les donnÃ©es en Train/Test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"âœ… DonnÃ©es prÃªtes avec {X_train.shape[1]} features !")

# ðŸ“Œ DÃ©finition des hyperparamÃ¨tres pour l'optimisation
param_grid = {
    "n_estimators": [50, 100, 200, 300],  # Nombre d'arbres
    "max_depth": [None, 10, 20, 30],  # Profondeur des arbres
    "min_samples_split": [2, 5, 10],  # Min. dâ€™Ã©chantillons pour diviser un nÅ“ud
}

# ðŸ“Œ Initialisation du modÃ¨le RandomForest
rf = RandomForestRegressor(random_state=42)

# ðŸ“Œ Optimisation avec RandomizedSearchCV
grid_search = RandomizedSearchCV(
    rf, param_grid, n_iter=10, cv=3, scoring="r2", n_jobs=-1, random_state=42
)
grid_search.fit(X_train, y_train)

# ðŸ“Œ Meilleur modÃ¨le trouvÃ©
best_model = grid_search.best_estimator_
print("âœ… Meilleurs paramÃ¨tres trouvÃ©s :", grid_search.best_params_)

# ðŸ“Œ PrÃ©dictions
y_pred = best_model.predict(X_test)

# ðŸ“Œ Ã‰valuation du modÃ¨le optimisÃ©
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"âœ… RÃ©sultats aprÃ¨s optimisation :")
print(f"MAE (Erreur absolue moyenne) : {mae:.2f}")
print(f"MSE (Erreur quadratique moyenne) : {mse:.2f}")
print(f"RÂ² Score : {r2:.4f}")

# ðŸ“Œ Sauvegarde du modÃ¨le optimisÃ©
joblib.dump(df.columns.tolist(), "model_columns.pkl")  # df = ton dataframe d'entraÃ®nement
joblib.dump(best_model, "car_price_predictor_optimized.pkl")
print("âœ… ModÃ¨le optimisÃ© sauvegardÃ© sous 'car_price_predictor_optimized.pkl' !")
